\section{Unconstrained Linear Quadratic Optimal Control}
In this section, DT LTI systems with quadratic cost function are treated, where the goal is to regulate the state to the origin, without any state or input constraints.
\subsection{Finite Horizon LQR}

The \textbf{unconstrained finite horizon control problem} has the objective of finding a sequence of inputs
\noindent\begin{equation*}
    U := \begin{bmatrix} u_0^\top & u_1^\top & \dots & u_{N-1}^\top \end{bmatrix}^\top
\end{equation*}
that minimizes the objective function:
\noindent\begin{align*}
    J^*(x(0))                & := \min_U J(x_0,U)                                                                          \\
                             & = \min_U \left( x_N^\top P x_N + \sum_{i=0}^{N-1} (x_i^\top Q x_i + u_i^\top R u_i) \right) \\
    \text{subj.\ to }x_{i+1} & = A x_i + B u_i, \quad i = 0, \dots, N-1                                                    \\
    x_0                      & = x(0)
\end{align*}
In practice, the optimization variables include also $x_0, x_1, \dots, x_N$.

\newpar{}
\ptitle{Weight Matrices}

\begin{itemize}
    \item $P \succeq 0$, with $P = P^\top$ is the terminal weight.
    \item $Q \succeq 0$, with $Q = Q^\top$ is the state weight.
    \item $R \succ 0$, with $R = R^\top$ is the input weight.
    \item $N$ is the horizon length.
\end{itemize}

\subsubsection{Batch Approach}
The batch approach yields a FFW series $U$ of numerical values for the input, as a function of $x(0)$. A constrained problem could theoretically still yield a solution through matrix inversion, which however might be unfeasible.

\newpar{}
\ptitle{Problem Setup}% TODO: Is this just the nitial condition response + forced response?

Defining
\noindent\begin{align*}
    X:                        & =S^x x(0)+S^u U \\
    \left.\left[
    \begin{array}
            {c}x_0 \\
            x_1    \\
            \vdots \\
            \vdots \\
            x_N
        \end{array}\right.\right] & =
    \begin{bmatrix}
        l      \\
        A      \\
        \vdots \\
        \vdots \\
        A^N
    \end{bmatrix}x(0)+
    \begin{bmatrix}
        0        & \cdots & \cdots & 0 \\
        B        & 0      & \cdots & 0 \\
        AB       & B      & \cdots & 0 \\
        \vdots   & \ddots & \ddots & 0 \\
        A^{N-1}B & \cdots & AB     & B
    \end{bmatrix}
    \begin{bmatrix}
        u_0    \\
        u_1    \\
        \vdots \\
        \vdots \\
        u_{N-1}
    \end{bmatrix}
\end{align*}
and the block diagonal weight matrices
\noindent\begin{equation*}
    \overline{Q}:=\mathrm{blockdiag}(Q,...,Q,P)\quad\mathrm{and}\quad\overline{R}:=\mathrm{blockdiag}(R,...,R)
\end{equation*}
the finite horizon cost $J(x(0), U) = X^{\top}\bar{Q}X + U^{\top}\bar{R}U$ becomes the positive definite quadratic function in $U$
\noindent\begin{gather*}
    J(x(0), U) =U^{\top}HU+2{x(0)}^{\top}FU+{x(0)}^{\top}S^{x^{\top}}\overline{Q}S^{x}x(0)\\
    \text{where } H:={(S^u)}^\top\overline{Q} S^u+\overline{R} > 0 \mathrm{~and~}F:={(S^x)}^\top\overline{Q} S^u
\end{gather*}

\newpar{}
\ptitle{Optimal Control Solution}

Setting the gradient to zero yields the optimal control action
\noindent\begin{equation*}
    U^*(x(0)) = -H^{-1}F^\top x(0)
\end{equation*}
Note that
\begin{itemize}
    \item This is linear in $x(0)$.
    \item $H^{-1}$ always exists as $H>0$ i.e.\ full rank.
\end{itemize}

\newpar{}
The \textbf{optimal cost} is

\noindent\begin{equation*}
    J^*(x(0)) = {x(0)}^{\top}\left({(S^{x})}^{\top}\overline{Q}S^{x}- F H^{-1} F^\top\right)x(0)
\end{equation*}

\subsubsection{Recursive Approach}
The recursive approach uses Dynamic Programming (DP) to compute an optimal policy, i.e.\ an optimal control action per state.

\paragraph{Bellman's Principle of Optimality}

Bellman's Principle of Optimality justifies the recursive approach, stating that:

\textit{For any solution for steps $0$ to $N$ to be optimal, any solution for steps $j$ to $N$ with $j \geq 0$, taken from the $0$ to $N$ solution, must itself be optimal for the $j$-to-$N$ problem.}

Hence, for any $j=0\dots N$
\begin{align*}
    J_{j}^{\star}(x_{j})     & =\min_{u_{j}}I(x_{i},u_{i})+J_{j+1}^{\star}(x_{j+1}) \\
    \text{subj.\ to }x_{j+1} & =Ax_{j}+Bu_{j}
\end{align*}

\paragraph{Optimal Control Method}

\ptitle{Problem Setup}

We define the $j$-step optimal cost-to-go (\textit{minimum} cost after step $j$) as the optimal cost attainable for the step $j$ problem
\noindent\begin{align*}
    J_j^\star(x(j)) & :=\min_{U_{j\to N}}x_N^\top Px_N+\sum_{i=j}^{N-1}(x_i^\top Qx_i+u_i^\top Ru_i) \\
                    & \text{subj.\ to }x_{i+1}=Ax_i+Bu_i,i=j,...,N-1                                 \\
                    & x_j=x(j)
\end{align*}

\newpar{}
\ptitle{Optimal Control Solution}

Solving the local subproblem yields the optimal control input
\noindent\begin{align*}
    u_{i}^{\star} & =F_{i} x_{i}                                   \\
    F_{i}         & :=-{(B^\top P_{i+1} B+R)}^{-1}B^\top P_{i+1} A
\end{align*}

The optimal cost-to-go is
\noindent\begin{equation*}
    J_{i}^{\star}(x_{i})  = x_{i}^{\top}P_{i}x_{i}
\end{equation*}
with cost matrices found from the Discrete Time Riccati equation / \textbf{Riccati Difference equation} (\textbf{RDE}):
\noindent\begin{align*}
     & P_{i} = A^\top P_{i+1} A+Q-A^\top P_{i+1} B{(B^\top P_{i+1} B+R)}^{-1}B^\top P_{i+1} A \\
     & P_{N} = P \text{ (init.\ with terminal weight)}
\end{align*}
and the full-trajectory \textbf{optimal cost} $J^*(x(0))$ is
\noindent\begin{equation*}
    {x(0)}^\top P_0 x(0)
\end{equation*}

\subsubsection{Comparison: Batch vs.\ Recursive Approach}

The recursive approach yields a policy instead of a fixed input sequence. Hence, in presence of \textbf{disturbances} or noise, the recursive approach is more robust. Additionally, the problem is broken down into smaller \textbf{subproblems}, avoiding large matrix inversions (Hessian in the batch approach). However, the batch approach is more favorable for \textbf{constrained} problems as constrained minimization can be performed.

\subsection{Receding Horizon LQR}

Receding horizon control calculates open-loop control sequences, yet introducing feedback through repeated calculation, which takes the actual state evolution into account.

\begin{center}
    \includegraphics[width = 0.8\linewidth]{receding_LQR.png}
\end{center}

\newpar{}
\ptitle{Stability of Finite Horizon Control Law}

Not only tuning of the weight matrices but also the horizon length can influence stability of the control law.

\begin{center}
    \includegraphics[width = 0.6\linewidth]{horizon_stability.png}
\end{center}

\subsection{Infinite Horizon LQR}

\ptitle{Problem Setup}

\begin{align*}
    J_\infty(x(0)) & =\min_{u(\cdot)}\sum_{i=0}^\infty\left(x_i^\top Qx_i+u_i^\top Ru_i\right) \\
                   & \text{subj.\ to }x_{i+1}=Ax_i+Bu_i,\quad i=0,1,2,...,\infty,              \\
                   & x_0=x(0)
\end{align*}

\newpar{}
\ptitle{Optimal Control Solution}

Solving the local subproblem yields the optimal control input
\begin{align*}
    u^\star(k) & = F_\infty x(k)                                       \\
    F_\infty   & := -{(B^\top P_\infty B+R)}^{-1}B^\top P_\infty Ax(k)
\end{align*}
The optimal cost-to-go
\begin{equation*}
    J_\infty(x(k))=x(k)^\top P_\infty x(k)
\end{equation*}
is the solution of the \textbf{Algebraic Riccati equation (ARE)} where $P_i = P_{i+1} = P_{\infty}$
\begin{equation*}
    P_\infty=A^\top P_\infty A+Q-A^\top P_\infty B{(B^\top P_\infty B+R)}^{-1}B^\top P_\infty A
\end{equation*}
is \textbf{unique} and \textbf{positive definite}.

\newpar{}
\ptitle{Technical Conditions}

Assuming
\begin{itemize}
    \item $(A,B)$ stabilizable
    \item $(Q^{\frac{1}{2}}, A)$ detectable,
\end{itemize}
then the RDE (initialized with $Q$ at $i = \infty$ and solved for $i\searrow 0$) converges to $P_{\infty}$.

\newpar{}
\ptitle{Stability}

If the technical conditions are fulfilled, the optimal value function $J^{\star} (x) = x^T P_{\infty} x$ is a Lyapunov function for the CL system $x^+ = (A + BF_{\infty} )x$, i.e.\ the system is asymptotically stable.





\section{Practical MPC}

\subsection{Reference Tracking}

A common task is to track a non-zero output set-point.

For a linear system with constraints, one can define the tracking task of the reference $r$ as
\begin{equation*}
    z(k) = Hx(k) \to r\in \mathbb{R}^{n_r} \text{ as } k \to \infty.
\end{equation*}
where $z(k)$ are the outputs that have to follow $r$.
\newpar{}
\textbf{Note}: The following concepts apply to piecewise constant references and not to general time-varying ones.

\subsubsection{Steady-State Target Problem}\label{steady_state_target_selection}

The reference is achieved by the target state $x_s$ if
\begin{equation*}
    z_s = Hx_s = r.
\end{equation*}
The target state should be a steady state, such that there exists an input that keeps the system at the target, i.e.
\begin{equation*}
    x_s = Ax_s +Bu_s
\end{equation*}
hence the target conditions become
\begin{equation*}
    \begin{array}{rcl}
        x_s  & = & Ax_s +Bu_s \\
        Hx_s & = & r
    \end{array}
    \Leftrightarrow
    \underbrace{\begin{bmatrix}
            I-A & -B \\
            H   & 0
        \end{bmatrix}}_{(n_x + n_r)\times(n_x + n_u)}
    \begin{bmatrix}
        x_s \\
        u_s
    \end{bmatrix}
    = \begin{bmatrix}
        0 \\
        r
    \end{bmatrix}
\end{equation*}
which can be easily solved for $(x_s, u_s)$.

\newpar{}
\ptitle{Multiple Solutions}

It is possible that there exist multiple feasible $(x_s, u_s)$ for a given reference $r$. Then the cheapest steady state can be used
\begin{align*}
    \min\quad    & u_s^\top R_s u_s                                \\
    \text{s.t. } & \begin{bmatrix}
                       I-A & -B \\
                       H   & 0
                   \end{bmatrix}
    \begin{bmatrix}
        x_s \\
        u_s
    \end{bmatrix}
    = \begin{bmatrix}
          0 \\
          r
      \end{bmatrix}                                               \\
                 & x_s \in \mathcal{X}, \quad u_s \in \mathcal{U}.
\end{align*}

\newpar{}
\ptitle{No Solution}

If no solution exists, then the reachable set point that is closest to $r$ can be computed
\begin{align*}
    \min\quad    & {\left(Hx_s -r\right)}^\top Q_s \left(Hx_s -r\right) \\
    \text{s.t. } & x_s = Ax_s + Bu_s                                    \\
                 & x_s \in \mathcal{X}, \quad u_s \in \mathcal{U}.
\end{align*}
Note that $u_s$ is unique in this case.

\subsubsection{Tracking Formulation}
A commonly-used formulation for tracking MPC cost is
\begin{equation*}
    \min_{U}\|z_{N}-Hx_{s}\|_{P_{z}}^{2}+\sum_{i=0}^{N-1}\|z_{i}-Hx_{s}\|_{Q_{z}}^{2}+\|u_{i}-u_{s}\|_{R}^{2}
\end{equation*}
% Q: Why does one (not) use it compared to delta?
% A: One thing: Delta formulation is DDP and this one is not (because of A*xs etc.)

\subsubsection{Delta-Formulation for Tracking}
By introducing the delta formulation the tracking problem is transformed to the origin and therefore can be treated like a normal MPC problem.

\newpar{}

The deviation variables are given by
\begin{gather*}
    \Delta x = x - x_s \\
    \Delta u = u - u_s
\end{gather*}
which results for a linear system in a system model
\begin{align*}
    \Delta x_{k+1} & = x_{k+1}-x_s                \\
                   & = Ax_k + Bu_k -(Ax_s + Bu_s) \\
                   & = A\Delta x_k + B\Delta u_k
\end{align*}
and the constraints become
\begin{align*}
    G_x x \leq h_x \Rightarrow & \quad G_x\Delta x \leq h_x - G_x x_s  \\
    G_u u \leq h_u \Rightarrow & \quad G_u\Delta u \leq h_u - G_u u_s.
\end{align*}

\begin{center}
    \includegraphics[width=\linewidth]{07_constraints_transform.png}
\end{center}

\newpar{}
\ptitle{Resulting Optimization Problem}

The complete problem in Delta-Formulation then looks like
\begin{align*}
    \Delta U^* = \arg\min_{\Delta U} \quad & \sum_{i=0}^{N-1} \Delta x_i^\top Q \Delta x_i + \Delta u_i^\top R \Delta u_i + V_f(\Delta x_N) \\
    \text{s.t.} \quad                      & \Delta x_0 = \Delta x(k) = x(k)-x_s                                                            \\
                                           & \Delta x_{i+1} = A \Delta x_i + B \Delta u_i                                                   \\
                                           & G_x \Delta x_i \leq h_x - G_x x_s                                                              \\
                                           & G_u \Delta u_i \leq h_u - G_u u_s                                                              \\
                                           & \Delta x_N \in \mathcal{X}_f
\end{align*}
where $x_s, u_s$ is assumed to be determined beforehand by~\ref{steady_state_target_selection}.

\begin{center}
    \includegraphics[width=0.7\linewidth]{07_delta-formulation.png}
\end{center}

where the final input to the system is
\begin{equation*}
    u_0^* = \Delta u_0^* + u_s.
\end{equation*}

\textbf{Note} that if the target steady-state is uniquely defined by the reference, we can also include the target condition as a constraint in the MPC problem.

\subsubsection{Convergence}

Assume target is feasible with $x_s \in \mathcal{X}, u_s \in \mathcal{U}$ and choose terminal weight $V_f(x)$ and constraint $\mathcal{X}_f$ as in the regulation case satisfying (i.e.\ we found $V_f(x), \mathcal{X}_f$ only for the regulation case):
\begin{align*}
    \mathcal{X}_f \subseteq \mathcal{X},        & \quad Kx \in \mathcal{U}      &  & \forall x \in \mathcal{X}_f \\
    V_f\bigl(x(k+1)\bigr) - V_f\bigl(x(k)\bigr) & \leq -I\bigl(x(k),Kx(k)\bigr) &  & \forall x \in \mathcal{X}_f
\end{align*}
\newpar{}
As the dynamics are identical in the Delta-formulation and the original system, the same Lyapunov function $V_f$ can be used.
\newpar{}
However, the terminal set constraint must be modified. In the Delta-formulation we require
\begin{equation*}
    \Delta x_N \in \mathcal{X}_f
\end{equation*}
which imposes
\begin{equation*}
    x_N \in \left\{x_s + \Delta x_N, \Delta x_N \in \mathcal{X}_f\right\} \subseteq \mathcal{X}
\end{equation*}
on the true system. Therefore the following condition is required: If in addition the target reference $x_s, u_s$ is such that
\begin{equation*}
    x_s \oplus \mathcal{X}_f \subseteq \mathcal{X}, \: K\Delta x + u_s \in \mathcal{U}, \quad \forall \Delta x \in \mathcal{X}_f
\end{equation*}
then the closed-loop system converges to the target reference, i.e.
\begin{equation*}
    x(k) \to x_s \text{ and therefore } z(k) = Hx(k) \to r \text{ for } k \to \infty.
\end{equation*}

\subsubsection{Terminal Set}

Due to the transformation the set of feasible targets may be significantly reduced.

\newpar{}

In the following example the blue line represents the state constraints and the green line the terminal set.

\newpar{}

\begin{tabularx}{\linewidth}{YY}
    Regulation case:                                          & Tracking using a shifted terminal set: \\

    \includegraphics[width=\linewidth]{07_terminal_set_1.png} &

    \includegraphics[width=\linewidth]{07_terminal_set_2.png}
\end{tabularx}
\begin{center}
    Set of feasible targets:

    \includegraphics[width=0.5\linewidth]{07_terminal_set_3.png}
\end{center}

\newpar{}
\ptitle{Terminal Set Scaling}

To enlarge the set of feasible targets one can scale the terminal set.
\begin{equation*}
    \mathcal{X}_f^{\text{scaled}} = \alpha \mathcal{X}_f
\end{equation*}
\begin{center}
    \includegraphics[width=0.5\linewidth]{07_terminal_set_4.png}
\end{center}

The scaling factor $\alpha$ has to be chosen such that the state and input constraints are still satisfied.
Targets at the boundary of the constraints hence $x_N = x_s$, correspond to a zero terminal set in the regulation case.
\newpar{}
\textbf{Note} that scaling preserves invariance.
\subsubsection{Offset-Free Reference Tracking}
Goal: Track constant reference $r$, i.e.\ $z(k) = Hy(k) \to r$ for $k \to \infty$. If system is stabilized in the presence of the disturbance then it converges to set point with zero offset.
\newpar{}
\ptitle{Algorithm}

At each sampling time:
\begin{enumerate}
    \item Estimate state and disturbance $\widehat{x}$, $\widehat{d}$
    \item Solve steady-state target problem using $\widehat{d}$
    \item Solve MPC problem:
\end{enumerate}
\begin{align*}
    \min_U       & \sum_{i=0}^{N-1} {(x_i - x_s)}^T Q (x_i - x_s)                                    \\
                 & + {(u_i - u_s)}^T R (u_i - u_s) + V_f(x_N - x_s)                                  \\
    \text{s.t. } & x_0 = \widehat{x}(k), \quad d_0 = \widehat{d}(k)                                  \\
                 & x_{i+1} = Ax_i + Bu_i + B_d d_i, \quad i = 0, \dots, N                            \\
                 & d_{i+1} = d_i, \quad i = 0, \dots, N                                              \\
                 & x_i \in \mathcal{X}, \quad u_i \in \mathcal{U}, \quad x_N - x_s \in \mathcal{X}_f
\end{align*}

% Duplicate
% \newpar{}
% \ptitle{Constant Disturbances}\label{const_dist}

% Constant disturbances cause the system trajectory to deviate from nominal dynamics. They act as follows on the true plant:
% \begin{align*}
%     x(k+1) & = Ax(k) + Bu(k) + B_d d \\
%     y(k)   & = Cx(k) + C_d d
% \end{align*}

\paragraph{Augmented Model}\label{const_dist}

The constant disturbance can be included in to the dynamic system model as follows:
\begin{align*}
    x_{k+1} & = Ax_k + Bu_k + B_d d_k \\
    d_{k+1} & = d_k                   \\
    y_k     & = Cx_k + C_d d_k
\end{align*}
with $d \in \mathbb{R}^{n_d}$.

\newpar{}
\ptitle{Observability of Augmented System}

The only restriction on the choice of $B_d, C_d$ is the observability of the augmented model.

\newpar{}
The augmented system is observable if and only if $(A, C)$ is observable and
\begin{equation*}
    \begin{bmatrix}
        A - I & B_d \\
        C     & C_d
    \end{bmatrix}
\end{equation*}
has full column rank, i.e.\ rank $= n_x + n_d$.
\newpar{}
Note that
\begin{itemize}
    \item The number of measured outputs must be large enough: $n_d \leq n_y$.
    \item This is known as Hautus observability condition (or PBH test).
\end{itemize}
\newpar{}
Intuitively, the condition can be explained as follows: At steady-state
\begin{equation*}
    \begin{bmatrix}
        A - I & B_d \\
        C     & C_d
    \end{bmatrix}
    \begin{bmatrix}
        x_s \\
        d_s
    \end{bmatrix}
    =
    \begin{bmatrix}
        0 \\
        y_s
    \end{bmatrix}
\end{equation*}
and given $y_s$, $d_s$ must be uniquely defined.

\paragraph{Linear State Estimation}
To improve the estimate of the disturbance and to account for non-measurable states, an observer can be used. For the augmented model we get
    {\small
        \begin{gather*}
            \begin{bmatrix}
                \widehat{x}(k + 1) \\
                \widehat{d}(k + 1)
            \end{bmatrix}=
            \begin{bmatrix}
                A & B_d \\
                0 & I
            \end{bmatrix}
            \begin{bmatrix}
                \widehat{x}(k) \\
                \widehat{d}(k)
            \end{bmatrix}
            +\begin{bmatrix}
                B \\
                0
            \end{bmatrix}
            u(k)
            \\
            +\begin{bmatrix}
                L_x \\
                L_d
            \end{bmatrix}
            \Bigl(-y(k) + C\widehat{x}(k) + C_d \widehat{d}(k)\Bigr)
        \end{gather*}
    }

where the last summand corrects the state \textbf{and} disturbance estimate. On convergence, this term vanishes, revealing the steady-state equations.
\newpar{}
The \textbf{error dynamics} are:
{\small
\begin{align*}
    \begin{bmatrix}
        \eta(k + 1) \\
        \eta_d (k+1)
    \end{bmatrix}
     & = \begin{bmatrix}
             x(k + 1) - \widehat{x}(k + 1) \\
             d(k + 1) - \widehat{d}(k + 1)
         \end{bmatrix}
    \\
     & =
    \left(
    \begin{bmatrix}
        A & B_d \\
        0 & I
    \end{bmatrix}
    +
    \begin{bmatrix}
        L_x \\
        L_d
    \end{bmatrix}
    \begin{bmatrix}
        C & C_d
    \end{bmatrix}
    \right)
    \begin{bmatrix}
        x(k) - \widehat{x}(k) \\
        d(k) - \widehat{d}(k)
    \end{bmatrix}
\end{align*}
}
\newpar{}
To obtain zero estimation error, the estimator gain $L = \begin{bmatrix} L_x & L_d \end{bmatrix}^\top$ must be chosen such that the error dynamics are asymptotically stable.

\newpar{}
\ptitle{Lemma: Offset-Free Steady-State Estimation}

Suppose the observer is asymptotically stable and $n_y = n_d$. The observer steady state satisfies:
\begin{equation*}
    \begin{bmatrix}
        A - I & B \\
        C     & 0
    \end{bmatrix}
    \begin{bmatrix}
        \widehat{x}_\infty \\
        u_\infty
    \end{bmatrix}
    =
    \begin{bmatrix}
        - B_d \widehat{d}_\infty \\
        y_\infty - C_d \widehat{d}_\infty
    \end{bmatrix}
\end{equation*}

In words: The observer output $C \widehat{x}_\infty + C_d \widehat{d}_\infty$ tracks $y_\infty$ without offset.

\paragraph{Steady-State Selection}
The knowledge of the disturbance estimate allows us to reformulate the steady-state selection problem as
\begin{align*}
    x_s & = Ax_s + Bu_s + B_d \widehat{d}_\infty \\
    z_s & = H(Cx_s + C_d \widehat{d}_\infty) = r
\end{align*}
which shows that both, the steady-state and the target are modified to account for the disturbance.
\newpar{}
As we don't have access to $\widehat{d}_\infty$, we use the best possible estimate for $\widehat{d}_\infty$, namely $\widehat{d}$ for the target selection problem, yielding
\begin{equation*}
    \begin{bmatrix}
        A - I & B \\
        HC    & 0
    \end{bmatrix}
    \begin{bmatrix}
        x_s \\
        u_s
    \end{bmatrix}
    =
    \begin{bmatrix}
        - B_d \widehat{d} \\
        r - HC_d \widehat{d}
    \end{bmatrix}
\end{equation*}

For regulation, we can simply set $r=0$.

\paragraph{Offset-free Tracking: Main Result}

Let $\kappa(\widehat{x}(k), \widehat{d}(k), r) = u_0^\star$ be the estimation-based control law.

Assume
\begin{itemize}
    \item $n_d = n_y$
    \item the RHC is recursively feasible and unconstrained for $k \geq j$
    \item and the closed-loop system
          \begin{align*}
              x(k+1)        =     & Ax(k) + B \kappa\bigl(\widehat{x}(k), \widehat{d}(k), r\bigr) + B_d d           \\
              \widehat{x}(k+1)  = & \bigl(A + L_x C\bigr) \widehat{x}(k) + \bigl(B_d + L_x C_d\bigr) \widehat{d}(k) \\
                                  & + B \kappa\bigl(\widehat{x}(k), \widehat{d}(k), r\bigr) - L_x y(k)              \\
              \widehat{d}(k+1)  = & L_d C \widehat{x}(k) + \bigl(I + L_d C_d\bigr) \widehat{d}(k) - L_d y(k)
          \end{align*}
          converges $\bigl(\widehat{x}(k)\rightarrow \widehat{x}_{\infty}$, $\widehat{d}(k)\rightarrow \widehat{d}_{\infty}$, $y(k)\rightarrow y_{\infty}$ for $k\rightarrow \infty\bigr)$.
\end{itemize}
Then $z(k) = Hy(k) \to r$ as $k \to \infty$, i.e.\ we track the reference in presence of the disturbance.

\subsection{Enlarging the Feasible Set}
\subsubsection{MPC Without Terminal Set}
% TODO: I find this subsubsection terrifyingly gfühlschmi-gspürsch mi, but I don't know how to formulate it more technically. I think in practice it is empirical anyways.
% Sounds like a Frazzolian "try everyting and see whats best" approach.
Using a terminal set constraint is attractive due to stability and recursive feasibility guarantees. However,
\begin{itemize}
    \item it reduces the feasible set
    \item potentially adds large number of extra constraints
    \item adds state constraints to problems with only input constraints
\end{itemize}
Hence, it would be beneficial to formulate an MPC without terminal constraint but yet with guaranteed stability.
\newpar{}
\ptitle{Conditions}

We can remove the terminal constraint while maintaining stability if
\begin{itemize}
    \item initial state lies in sufficiently small subset of feasible set
    \item $N$ is sufficiently large
\end{itemize}
such that the terminal state satisfies terminal constraint without enforcing it in the optimization.
\newpar{}
In that case, the solution of the finite horizon MPC problem corresponds to the infinite horizon solution.

\newpar{}
\ptitle{Properties}

\begin{itemize}
    \item [+] Controller defined in a larger feasible set
    \item [-] Characterization of ROA or specification of required $N$ extremely difficult
\end{itemize}

\newpar{}
\ptitle{Method}

Enlarge horizon and check stability by \textbf{sampling}.
\newpar{}
With larger horizon length N, region of attraction approaches maximum control invariant set.

\subsubsection{Soft Constrained MPC}
In practice, input constraints are often ``hard'' whereas state constraints can usually be relaxed.
\newpar{}
If we decide to tolerate state constraint violations, we can either minimize their
\begin{itemize}
    \item duration
    \item or size
\end{itemize}
These goals can be conflicting and their relative importance depends on the application. The optimum lies on a Bambachian pareto curve. Operating exactly on this curve is usually difficult and only approximated. % TODO: sorry

\newpar{}
\ptitle{Properties}

\begin{itemize}
    \item [-] Standard methods do \textbf{not provide a stability guarantee} for OL unstable systems
\end{itemize}

\paragraph{Problem Setup}
In soft constrained MPC we
\begin{itemize}
    \item relax state constraints using slack variables $\epsilon_i \in \mathbb{R}^p$
    \item penalize constraint violation in cost using $l_\epsilon(\epsilon_i)$
\end{itemize}
The MPC can then be formulated as
\begin{align*}
    \min_u \quad      & \sum_{i=0}^{N-1} x_i^\top Q x_i + u_i^\top R u_i + {\color{red}\ell_\epsilon(\epsilon_i)} + x_N^\top P x_N + {\color{red}\ell_\epsilon(\epsilon_N)} \\
    \text{s.t.} \quad & x_{i+1} = A x_i + B u_i                                                                                                                             \\
                      & H_x x_i \leq k_x + {\color{red}\epsilon_i}                                                                                                          \\
                      & H_u u_i \leq k_u                                                                                                                                    \\
                      & {\color{red}\epsilon_i \geq 0}
\end{align*}

\paragraph{Penalty Function Choices}
\ptitle{Exact Penalty}

The penalty function $\ell_\epsilon(\epsilon_i)$ should be as follows: If the original problem has a feasible solution $z^\star$, then the softened problem should have the same solution and $\epsilon = 0$.
\newpar{}
\ptitle{Main Result}
\begin{equation*}
    \ell_\epsilon(\epsilon) = v \cdot \epsilon
\end{equation*}
satisfies the requirement for any $v > \lambda^\star \geq 0$, where $\lambda^\star$ is the optimal Lagrange multiplier for the original problem.
\newpar{}
\ptitle{Practical Penalty}

Combine linear and quadratic terms for tuning:
\begin{equation*}
    \ell_\epsilon(\epsilon) = v \cdot \epsilon + s \cdot \epsilon^2
\end{equation*}
with $v > \lambda^\star$ and $s > 0$.

\newpar{}
\ptitle{Extension to Multiple Constraints}

Assume multiple constraints $g_j(z) \leq 0$, $j = 1, \dots, r$. The penalty then reads
\begin{equation*}
    \ell_\epsilon(\epsilon) = v \cdot \lVert \epsilon \rVert_{1/\infty} + \epsilon^\top S \epsilon
\end{equation*}
% TODO: does 1/infinity mean one of them?
where
\begin{itemize}
    \item $\epsilon = {[\epsilon_1, \dots, \epsilon_r]}^\top$,
    \item $v > \lVert \lambda^\star \rVert_D$,
    \item and $S \succ 0$ can be used to weight violations differently.
\end{itemize}
\newpar{}
Note that $\lVert \cdot \rVert_D$ denotes the dual norm.

\newpar{}
\ptitle{Comparison of Penalty Functions}

The following properties hold for quadratic and linear penalties respectively
\begin{itemize}
    \item Quadratic:
          \begin{itemize}
              \item [+] Well-posed QP (positive definite Hessian)
              \item Increase in $S$ hardens soft constraints
              \item [-] \textbf{Not} exact for any choice of $s>0$
          \end{itemize}
    \item Linear:
          \begin{itemize}
              \item [+] Allows for exact penalties if $v$ large enough
              \item [+] If $v$ is large enough, constraints satisfied if possible
              \item [-] Large $v$ makes tuning hard and causes numerical issues
          \end{itemize}
\end{itemize}

\begin{examplesection}[Comparison of Penalty Functions]
    \begin{center}
        \includegraphics[width = \linewidth]{07_penalties.png}
    \end{center}

    $g(z) \triangleq z - z^\star \leq 0$ induces feasible region (grey). Hence, the minimizer of the original problem is $z^\star$

    \begin{itemize}
        \item Quadratic penalty:
              \begin{gather*}
                  l_\epsilon(\epsilon) = s \cdot \epsilon^2 \text{ for } \epsilon \geq 0,\\
                  l_\epsilon(\epsilon) = 0, \text{ otherwise}
              \end{gather*}
              $\rightarrow$ minimizer of $f(z) + l_\epsilon(\epsilon)$ is $(z^\star + \epsilon^\star, \epsilon^\star)$ instead of $(z^\star, 0)$
        \item Linear penalty:
              \begin{gather*}
                  l_\epsilon(\epsilon) = v \cdot \epsilon \text{ for }\epsilon \geq 0,\\
                  l_\epsilon(\epsilon) = 0, \text{ otherwise},
              \end{gather*}
              with $v$ such that $v + \lim_{z \to z^\star} f'(z) > 0$
              \newpar{}
              $\rightarrow$ minimizer of $f(z) + l_\epsilon(\epsilon)$ is $(z^\star, 0)$
    \end{itemize}
\end{examplesection}

\paragraph{Simplification: Separation of Objectives}
\ptitle{Step 1: Minimize Violation}
\begin{align*}
    \min_{u, \epsilon} \quad & \sum_{i=0}^{N-1} \epsilon_i^\top S \epsilon_i + v^\top \epsilon_i \\
    \text{s.t.} \quad        & x_{i+1} = A x_i + B u_i                                           \\
                             & H_x x_i \leq k_x + \epsilon_i                                     \\
                             & H_u u_i \leq k_u                                                  \\
                             & \epsilon_i \geq 0
\end{align*}
\newpar{}
\ptitle{Step 2: Optimize Performance}

\begin{align*}
    \min_u \quad      & \sum_{i=0}^{N-1} x_i^\top Q x_i + u_i^\top R u_i + x_N^\top P x_N \\
    \text{s.t.} \quad & x_{i+1} = A x_i + B u_i                                           \\
                      & H_x x_i \leq k_x + \epsilon_i^{\min}                              \\
                      & H_u u_i \leq k_u
\end{align*}

\newpar{}
\ptitle{Properties}

\begin{itemize}
    \item [+] Simplifies tuning, constraints satisfied if possible
    \item [-] Requires solving two optimization problems
\end{itemize}

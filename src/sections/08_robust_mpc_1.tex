\section{Robust MPC}
We assume an uncertain constrained system of the form:
\begin{gather*}
    x(k + 1) = g(x(k), u(k), w(k); \theta),\\
    x, u \in \mathcal{X}, \mathcal{U}, \quad w \in \mathcal{W}, \quad \theta \in \Theta
\end{gather*}
where
\begin{itemize}
    \item $w$ is random noise, changing with time, influencing system evolution
    \item $\theta$ are unknown, constant or slowly-varying parameters that impact the dynamics
\end{itemize}
\newpar{}
\ptitle{Goals of Robust Constrained Control}

Design control law $u(k) = \kappa(x(k))$ such that the system:
\begin{itemize}
    \item Satisfies constraints: $\{x(k)\} \subset \mathcal{X}, \{u(k)\} \subset \mathcal{U}$ for all disturbance realizations
    \item Is stable: converges to a neighborhood of the origin
    \item Optimizes (expected/worst-case) ``performance''
    \item Maximizes the set $\{x(0) \mid \text{ Conditions 1-3 are met }\}$
\end{itemize}

\subsection{Modeling Uncertainty}
\subsubsection{Common Uncertainty Models}
\ptitle{Measurement / Input Bias}

\begin{equation*}
    g(x(k), u(k), w(k); \theta) = \tilde{g}(x(k), u(k)) + \theta
\end{equation*}
$\theta$ unknown, but constant. Note that we called $\theta = d$ in~\ref{const_dist}.

\newpar{}
\ptitle{Linear Parameter Varying System}

{\small
    \begin{equation*}
        g(x(k), u(k), \theta(k)) =
        \left( \sum_{j=0}^{n_\theta} \theta_j(k) A_j \right)x(k) +
        \left( \sum_{j=0}^{n_\theta} \theta_j(k) B_j \right)u(k)
    \end{equation*}
}
time-varying parameters $\theta(k)$ describe a convex combination of $A_j , B_j$ with $\mathbf{1}^\top \theta(k) = 1$, $\theta(k) \geq 0$.

\newpar{}
\ptitle{Additive Stochastic Noise}

\begin{equation*}
    g(x(k), u(k), w(k); \theta) = Ax(k) + Bu(k) + w(k)
\end{equation*}
where $w$ comes from a known distribution. Used in stochastic MPC.

\newpar{}
\ptitle{Additive Bounded Noise}

In this course, the following noise models are considered:
\begin{equation*}
    g(x(k), u(k), w(k); \theta) = Ax(k) + Bu(k) + w(k), \quad w \in \mathcal{W}
\end{equation*}
where $A,B$ are known, $w$ is unknown but \textbf{bounded} and changing at each sampling instance. However, in contrast to the stochastic noise case, $w$ now comes from a certain discrete set and not from a distribution.
\newpar{}
Note that
\begin{itemize}
    \item we may model many nonlinearities in this fashion, but often a conservative model
    \item the noise is \textit{persistent}, i.e., it does not converge to zero in the limit
\end{itemize}

\subsubsection{Robust Invariant Sets}
\ptitle{Robust Positive Invariant Set}

A set $\mathcal{O}_\mathcal{W}$ is said to be a robust positive invariant set for the autonomous system $x(k + 1) = g(x(k), w(k))$ if
\begin{equation*}
    x \in \mathcal{O}_\mathcal{W} \Rightarrow g(x, w) \in \mathcal{O}_\mathcal{W}, \quad \forall w \in \mathcal{W}
\end{equation*}
The same concept is used for CL systems $x(k+1)=g(x(k), \kappa(x(k), w(k)))$.

\newpar{}
\ptitle{Robust Invariant Set Conditions}

Theorem: Geometric condition for robust invariance

A set $\mathcal{O}_\mathcal{W}$ is a robust positive invariant set if and only if
\begin{equation*}
    \mathcal{O}_\mathcal{W} \subseteq \text{pre}^{\mathcal{W}}(\mathcal{O}_\mathcal{W})
\end{equation*}

\newpar{}
\ptitle{Computing Robust Invariant Sets}

Similar to~\ref{ssec:computing_invariant_sets}:
\begin{algorithmic}
    \State{} \textbf{Input:} $g, \mathcal{X}, \mathcal{W}$
    \State{} \textbf{Output:} $\mathcal{O}_\infty^\mathcal{W}$
    \State{} $\Omega_0 \gets X$
    \While{true}
    \State{} $\Omega_{i+1} \gets \text{pre}^\mathcal{W}(\Omega_i) \cap \Omega_i$
    \If{$\Omega_{i+1} = \Omega_i$}
    \State{} \textbf{return} $\mathcal{O}_\infty^\mathcal{W} = \Omega_i$
    \EndIf{}
    \EndWhile{}
\end{algorithmic}
where $\Omega_0$ is chosen so that it is as large as possible, choosing any $w\in \mathcal{W}$.

\paragraph{Robust Pre-Sets}

Given a set $\Omega$ and dynamics $x(k + 1) = g(x(k), w(k))$, the pre-set of $\Omega$ is is the set of states that evolve into the target set $\Omega$ in one time step \textbf{for all values} of the disturbance $w \in \mathcal{W}$:
\begin{equation*}
    \text{pre}^{\mathcal{W}}(\Omega) := \{x \mid g(x, w) \in \Omega, \; \forall w \in \mathcal{W} \}
\end{equation*}

Each state is now mapped into an entire possible set of next states:
\begin{center}
    \includegraphics[width = 0.5\linewidth]{8_rob_preset.png}
\end{center}

\newpar{}
\ptitle{Computing Robust Pre-Sets for Linear Systems}

Let $g(x(k), w(k)) = Ax(k) + w(k)$ and $\Omega := \{x \mid Fx \leq f\}$. Then, fulfilling the state constraints can be seen as parallel displacement of the constraint boundaries
\begin{align*}
    \text{pre}_{\mathcal{W}}(\Omega) & = \{x \mid Ax + w \in \Omega, \; \forall w \in \mathcal{W} \}                             \\
                                     & = \{x \mid F_j Ax \leq f_j -F_j w, \; \forall w \in \mathcal{W} \}                        \\
                                     & = \{x \mid F_j Ax \leq f_j -\max_{w\in \mathcal{W}} F_j , \; \forall w \in \mathcal{W} \} \\
                                     & = \{x \mid FAx \leq f - h_\mathcal{W}(F) \}
\end{align*}
where $h_\mathcal{W}$ is the support function.
\begin{center}
    \includegraphics[width = 0.8\linewidth]{8_rob_inv_for_LTI.png}
\end{center}

\newpar{}
\ptitle{Minkowski Sum and Pontryagin Difference}

Let $A,B$ be subsets of $\mathbb{R}^n$. The Minkowski Sum is:
\begin{equation*}
    A \oplus B := \{x + y \mid x \in A, y \in B\}
\end{equation*}
The Pontryagin Difference is:
\begin{equation*}
    A \ominus B := \{x \mid x + e \in A, \forall e \in B\}
\end{equation*}
\begin{center}
    \includegraphics[width = \linewidth]{8_Pontryagin.png}
\end{center}

